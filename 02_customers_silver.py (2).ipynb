{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a630bc16-a95e-4496-8447-2c09bf622855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570ff80a-078c-4cb2-a9d5-f33bec846075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFAF RUNNING COMPLETE IMPORT PIPELINE\n============================================================\n\uD83D\uDCCA Starting table size: 251 rows\n\uD83D\uDCE5 Found 405 new files:\n   - enhanced_20250806_113101.json\n   - enhanced_20250806_113103.json\n   - enhanced_20250806_113105.json\n   - enhanced_20250806_113108.json\n   - enhanced_20250806_113110.json\n   - enhanced_20250806_113113.json\n   - enhanced_20250806_113115.json\n   - enhanced_20250806_113118.json\n   - enhanced_20250806_113120.json\n   - enhanced_20250806_113123.json\n   - enhanced_20250806_113125.json\n   - enhanced_20250806_113127.json\n   - enhanced_20250806_113130.json\n   - enhanced_20250806_113132.json\n   - enhanced_20250806_113135.json\n   - enhanced_20250806_113137.json\n   - enhanced_20250806_113140.json\n   - enhanced_20250806_113142.json\n   - enhanced_20250806_113144.json\n   - enhanced_20250806_113147.json\n   - enhanced_20250806_113149.json\n   - enhanced_20250806_113151.json\n   - enhanced_20250806_113154.json\n   - enhanced_20250806_113156.json\n   - enhanced_20250806_113159.json\n   - enhanced_20250806_113201.json\n   - enhanced_20250806_113203.json\n   - enhanced_20250806_113206.json\n   - enhanced_20250806_113208.json\n   - enhanced_20250806_113210.json\n   - enhanced_20250806_113213.json\n   - enhanced_20250806_113215.json\n   - enhanced_20250806_113218.json\n   - enhanced_20250806_113220.json\n   - enhanced_20250806_113222.json\n   - enhanced_20250806_113225.json\n   - enhanced_20250806_113227.json\n   - enhanced_20250806_113230.json\n   - enhanced_20250806_113232.json\n   - enhanced_20250806_113235.json\n   - enhanced_20250806_113237.json\n   - enhanced_20250806_113239.json\n   - enhanced_20250806_113242.json\n   - enhanced_20250806_113244.json\n   - enhanced_20250806_113247.json\n   - enhanced_20250806_113249.json\n   - enhanced_20250806_113252.json\n   - enhanced_20250806_113254.json\n   - enhanced_20250806_113256.json\n   - enhanced_20250806_113259.json\n   - enhanced_20250806_113301.json\n   - enhanced_20250806_113303.json\n   - enhanced_20250806_113306.json\n   - enhanced_20250806_113308.json\n   - enhanced_20250806_113311.json\n   - enhanced_20250806_113313.json\n   - enhanced_20250806_113316.json\n   - enhanced_20250806_113318.json\n   - enhanced_20250806_113320.json\n   - enhanced_20250806_113323.json\n   - enhanced_20250806_113325.json\n   - enhanced_20250806_113327.json\n   - enhanced_20250806_113330.json\n   - enhanced_20250806_113332.json\n   - enhanced_20250806_113334.json\n   - enhanced_20250806_113337.json\n   - enhanced_20250806_113339.json\n   - enhanced_20250806_113341.json\n   - enhanced_20250806_113343.json\n   - enhanced_20250806_113346.json\n   - enhanced_20250806_113348.json\n   - enhanced_20250806_113350.json\n   - enhanced_20250806_113353.json\n   - enhanced_20250806_113355.json\n   - enhanced_20250806_113358.json\n   - enhanced_20250806_113400.json\n   - enhanced_20250806_113403.json\n   - enhanced_20250806_113405.json\n   - enhanced_20250806_113408.json\n   - enhanced_20250806_113410.json\n   - enhanced_20250806_113413.json\n   - enhanced_20250806_113415.json\n   - enhanced_20250806_113417.json\n   - enhanced_20250806_113420.json\n   - enhanced_20250806_113422.json\n   - enhanced_20250806_113425.json\n   - enhanced_20250806_113427.json\n   - enhanced_20250806_113430.json\n   - enhanced_20250806_113432.json\n   - enhanced_20250806_113434.json\n   - enhanced_20250806_113437.json\n   - enhanced_20250806_113439.json\n   - enhanced_20250806_113442.json\n   - enhanced_20250806_113444.json\n   - enhanced_20250806_113447.json\n   - enhanced_20250806_113449.json\n   - enhanced_20250806_113452.json\n   - enhanced_20250806_113454.json\n   - enhanced_20250806_113457.json\n   - enhanced_20250806_113459.json\n   - enhanced_20250806_113502.json\n   - enhanced_20250806_113505.json\n   - enhanced_20250806_113507.json\n   - enhanced_20250806_113510.json\n   - enhanced_20250806_113512.json\n   - enhanced_20250806_113515.json\n   - enhanced_20250806_113518.json\n   - enhanced_20250806_113520.json\n   - enhanced_20250806_113523.json\n   - enhanced_20250806_113525.json\n   - enhanced_20250806_113528.json\n   - enhanced_20250806_113530.json\n   - enhanced_20250806_113533.json\n   - enhanced_20250806_113535.json\n   - enhanced_20250806_113538.json\n   - enhanced_20250806_113541.json\n   - enhanced_20250806_113543.json\n   - enhanced_20250806_113546.json\n   - enhanced_20250806_113548.json\n   - enhanced_20250806_113551.json\n   - enhanced_20250806_113553.json\n   - enhanced_20250806_113556.json\n   - enhanced_20250806_113558.json\n   - enhanced_20250806_113601.json\n   - enhanced_20250806_113603.json\n   - enhanced_20250806_113606.json\n   - enhanced_20250806_113609.json\n   - enhanced_20250806_113611.json\n   - enhanced_20250806_113614.json\n   - enhanced_20250806_113616.json\n   - enhanced_20250806_113619.json\n   - enhanced_20250806_113621.json\n   - enhanced_20250806_113624.json\n   - enhanced_20250806_113626.json\n   - enhanced_20250806_113629.json\n   - enhanced_20250806_113631.json\n   - enhanced_20250806_113634.json\n   - enhanced_20250806_113636.json\n   - enhanced_20250806_113639.json\n   - enhanced_20250806_113641.json\n   - enhanced_20250806_113644.json\n   - enhanced_20250806_113646.json\n   - enhanced_20250806_113649.json\n   - enhanced_20250806_113651.json\n   - enhanced_20250806_113654.json\n   - enhanced_20250806_113656.json\n   - enhanced_20250806_113659.json\n   - enhanced_20250806_113701.json\n   - enhanced_20250806_113703.json\n   - enhanced_20250806_113706.json\n   - enhanced_20250806_113708.json\n   - enhanced_20250806_113711.json\n   - enhanced_20250806_113713.json\n   - enhanced_20250806_113716.json\n   - enhanced_20250806_113718.json\n   - enhanced_20250806_113721.json\n   - enhanced_20250806_113723.json\n   - enhanced_20250806_113726.json\n   - enhanced_20250806_113728.json\n   - enhanced_20250806_113731.json\n   - enhanced_20250806_113734.json\n   - enhanced_20250806_113736.json\n   - enhanced_20250806_113739.json\n   - enhanced_20250806_113741.json\n   - enhanced_20250806_113743.json\n   - enhanced_20250806_113746.json\n   - enhanced_20250806_113748.json\n   - enhanced_20250806_113751.json\n   - enhanced_20250806_113753.json\n   - enhanced_20250806_113756.json\n   - enhanced_20250806_113758.json\n   - enhanced_20250806_113801.json\n   - enhanced_20250806_113803.json\n   - enhanced_20250806_113806.json\n   - enhanced_20250806_113808.json\n   - enhanced_20250806_113811.json\n   - enhanced_20250806_113814.json\n   - enhanced_20250806_113816.json\n   - enhanced_20250806_113818.json\n   - enhanced_20250806_113821.json\n   - enhanced_20250806_113823.json\n   - enhanced_20250806_113826.json\n   - enhanced_20250806_113828.json\n   - enhanced_20250806_113831.json\n   - enhanced_20250806_113833.json\n   - enhanced_20250806_113835.json\n   - enhanced_20250806_113838.json\n   - enhanced_20250806_113841.json\n   - enhanced_20250806_113843.json\n   - enhanced_20250806_113846.json\n   - enhanced_20250806_113848.json\n   - enhanced_20250806_113851.json\n   - enhanced_20250806_113853.json\n   - enhanced_20250806_113855.json\n   - enhanced_20250806_113858.json\n   - enhanced_20250806_113900.json\n   - enhanced_20250806_113903.json\n   - enhanced_20250806_113905.json\n   - enhanced_20250806_113907.json\n   - enhanced_20250806_113910.json\n   - enhanced_20250806_113912.json\n   - enhanced_20250806_113915.json\n   - enhanced_20250806_113917.json\n   - enhanced_20250806_113919.json\n   - enhanced_20250806_113922.json\n   - enhanced_20250806_113924.json\n   - enhanced_20250806_113926.json\n   - enhanced_20250806_113929.json\n   - enhanced_20250806_113931.json\n   - enhanced_20250806_113934.json\n   - enhanced_20250806_113936.json\n   - enhanced_20250806_113938.json\n   - enhanced_20250806_113941.json\n   - enhanced_20250806_113943.json\n   - enhanced_20250806_113946.json\n   - enhanced_20250806_113948.json\n   - enhanced_20250806_113950.json\n   - enhanced_20250806_113953.json\n   - enhanced_20250806_113955.json\n   - enhanced_20250806_113958.json\n   - enhanced_20250806_114000.json\n   - enhanced_20250806_114002.json\n   - enhanced_20250806_114005.json\n   - enhanced_20250806_114007.json\n   - enhanced_20250806_114010.json\n   - enhanced_20250806_114012.json\n   - enhanced_20250806_114014.json\n   - enhanced_20250806_114016.json\n   - enhanced_20250806_114019.json\n   - enhanced_20250806_114021.json\n   - enhanced_20250806_114024.json\n   - enhanced_20250806_114026.json\n   - enhanced_20250806_114028.json\n   - enhanced_20250806_114031.json\n   - enhanced_20250806_114033.json\n   - enhanced_20250806_114035.json\n   - enhanced_20250806_114038.json\n   - enhanced_20250806_114040.json\n   - enhanced_20250806_114042.json\n   - enhanced_20250806_114045.json\n   - enhanced_20250806_114047.json\n   - enhanced_20250806_114049.json\n   - enhanced_20250806_114052.json\n   - enhanced_20250806_114054.json\n   - enhanced_20250806_114057.json\n   - enhanced_20250806_114059.json\n   - enhanced_20250806_114102.json\n   - enhanced_20250806_114104.json\n   - enhanced_20250806_114107.json\n   - enhanced_20250806_114109.json\n   - enhanced_20250806_114111.json\n   - enhanced_20250806_114114.json\n   - enhanced_20250806_114116.json\n   - enhanced_20250806_114118.json\n   - enhanced_20250806_114121.json\n   - enhanced_20250806_114123.json\n   - enhanced_20250806_114126.json\n   - enhanced_20250806_114128.json\n   - enhanced_20250806_114131.json\n   - enhanced_20250806_114133.json\n   - enhanced_20250806_114136.json\n   - enhanced_20250806_114138.json\n   - enhanced_20250806_114141.json\n   - enhanced_20250806_114143.json\n   - enhanced_20250806_114146.json\n   - enhanced_20250806_114148.json\n   - enhanced_20250806_114151.json\n   - enhanced_20250806_114154.json\n   - enhanced_20250806_114156.json\n   - enhanced_20250806_114159.json\n   - enhanced_20250806_114202.json\n   - enhanced_20250806_114204.json\n   - enhanced_20250806_114207.json\n   - enhanced_20250806_114210.json\n   - enhanced_20250806_114212.json\n   - enhanced_20250806_114215.json\n   - enhanced_20250806_114217.json\n   - enhanced_20250806_114220.json\n   - enhanced_20250806_114222.json\n   - enhanced_20250806_114225.json\n   - enhanced_20250806_114227.json\n   - enhanced_20250806_114230.json\n   - enhanced_20250806_114232.json\n   - enhanced_20250806_114235.json\n   - enhanced_20250806_114237.json\n   - enhanced_20250806_114240.json\n   - enhanced_20250806_114242.json\n   - enhanced_20250806_114245.json\n   - enhanced_20250806_114248.json\n   - enhanced_20250806_114250.json\n   - enhanced_20250806_114253.json\n   - enhanced_20250806_114255.json\n   - enhanced_20250806_114258.json\n   - enhanced_20250806_114300.json\n   - enhanced_20250806_114303.json\n   - enhanced_20250806_114305.json\n   - enhanced_20250806_114308.json\n   - enhanced_20250806_114310.json\n   - enhanced_20250806_114313.json\n   - enhanced_20250806_114315.json\n   - enhanced_20250806_114318.json\n   - enhanced_20250806_114320.json\n   - enhanced_20250806_114323.json\n   - enhanced_20250806_114326.json\n   - enhanced_20250806_114328.json\n   - enhanced_20250806_114330.json\n   - enhanced_20250806_114333.json\n   - enhanced_20250806_114335.json\n   - enhanced_20250806_114338.json\n   - enhanced_20250806_114340.json\n   - enhanced_20250806_114343.json\n   - enhanced_20250806_114345.json\n   - enhanced_20250806_114347.json\n   - enhanced_20250806_114350.json\n   - enhanced_20250806_114352.json\n   - enhanced_20250806_114355.json\n   - enhanced_20250806_114357.json\n   - enhanced_20250806_114359.json\n   - enhanced_20250806_114402.json\n   - enhanced_20250806_114404.json\n   - enhanced_20250806_114407.json\n   - enhanced_20250806_114409.json\n   - enhanced_20250806_114412.json\n   - enhanced_20250806_114414.json\n   - enhanced_20250806_114417.json\n   - enhanced_20250806_114419.json\n   - enhanced_20250806_114422.json\n   - enhanced_20250806_114424.json\n   - enhanced_20250806_114427.json\n   - enhanced_20250806_114429.json\n   - enhanced_20250806_114432.json\n   - enhanced_20250806_114434.json\n   - enhanced_20250806_114437.json\n   - enhanced_20250806_114439.json\n   - enhanced_20250806_114441.json\n   - enhanced_20250806_114444.json\n   - enhanced_20250806_114446.json\n   - enhanced_20250806_114449.json\n   - enhanced_20250806_114451.json\n   - enhanced_20250806_114453.json\n   - enhanced_20250806_114456.json\n   - enhanced_20250806_114458.json\n   - enhanced_20250806_114500.json\n   - enhanced_20250806_114503.json\n   - enhanced_20250806_114505.json\n   - enhanced_20250806_114508.json\n   - enhanced_20250806_114510.json\n   - enhanced_20250806_114513.json\n   - enhanced_20250806_114515.json\n   - enhanced_20250806_114518.json\n   - enhanced_20250806_114520.json\n   - enhanced_20250806_114523.json\n   - enhanced_20250806_114525.json\n   - enhanced_20250806_114528.json\n   - enhanced_20250806_114530.json\n   - enhanced_20250806_114533.json\n   - enhanced_20250806_114535.json\n   - enhanced_20250806_114538.json\n   - enhanced_20250806_114540.json\n   - enhanced_20250806_114543.json\n   - enhanced_20250806_114545.json\n   - enhanced_20250806_114547.json\n   - enhanced_20250806_114550.json\n   - enhanced_20250806_114552.json\n   - enhanced_20250806_114554.json\n   - enhanced_20250806_114557.json\n   - enhanced_20250806_114559.json\n   - enhanced_20250806_114602.json\n   - enhanced_20250806_114604.json\n   - enhanced_20250806_114607.json\n   - enhanced_20250806_114609.json\n   - enhanced_20250806_114611.json\n   - enhanced_20250806_114614.json\n   - enhanced_20250806_114616.json\n   - enhanced_20250806_114619.json\n   - enhanced_20250806_114621.json\n   - enhanced_20250806_114624.json\n   - enhanced_20250806_114626.json\n   - enhanced_20250806_114629.json\n   - enhanced_20250806_114631.json\n   - enhanced_20250806_114634.json\n   - enhanced_20250806_114636.json\n   - enhanced_20250806_114638.json\n   - enhanced_20250806_114641.json\n   - enhanced_20250806_114643.json\n   - enhanced_20250806_114646.json\n   - enhanced_20250806_114649.json\n   - enhanced_20250806_114652.json\n   - enhanced_20250806_114654.json\n   - enhanced_20250806_114657.json\n   - enhanced_20250806_114659.json\n   - enhanced_20250806_114701.json\n   - enhanced_20250806_114704.json\n   - enhanced_20250806_114706.json\n   - enhanced_20250806_114708.json\n   - enhanced_20250806_114711.json\n   - enhanced_20250806_114713.json\n   - enhanced_20250806_114716.json\n   - enhanced_20250806_114718.json\n   - enhanced_20250806_114720.json\n   - enhanced_20250806_114723.json\n   - enhanced_20250806_114725.json\n   - enhanced_20250806_114728.json\n   - enhanced_20250806_114730.json\n   - enhanced_20250806_114733.json\n\uD83D\uDE80 IMPORTING ENHANCED DATA (SCHEMA ALIGNED)\n============================================================\n\uD83D\uDCE5 Found 405 new files\n\uD83D\uDD0D 195 existing post IDs\n\uD83D\uDCCA Total downloaded: 7263\n\uD83C\uDD95 New unique posts: 6555\n\uD83D\uDD04 Duplicates filtered: 708\n✅ DataFrame prepared: 6555 rows\n\uD83D\uDCCB Using 24 matching columns\n✅ Spark DataFrame created with matching schema\n\uD83D\uDCBE Appending to Delta table...\n✅ 6,555 rows appended successfully!\n✅ 10 files moved to processed\n\n============================================================\n\uD83C\uDF89 PIPELINE COMPLETE\n============================================================\n\uD83D\uDCCA DELTA TABLE CHANGES:\n   ├── Starting rows: 251\n   ├── NEW rows added: 6,555  ⭐⭐⭐\n   └── Final rows: 6,806\n\uD83D\uDCC8 Table growth: +2611.6%\n✅ Files processed: 405\n============================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'new_rows_added': 6555,\n",
       " 'starting_rows': 251,\n",
       " 'final_rows': 6806,\n",
       " 'files_processed': 405}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 1: CHECK FOR NEW FILES\n",
    "# =============================================================================\n",
    "\n",
    "def check_new_files():\n",
    "    \"\"\"Check GitHub for new enhanced files to process\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    GITHUB_USERNAME = \"AlexanderHuynhKoehler\"\n",
    "    REPO_NAME = \"bluesky-data-pipeline\"\n",
    "    api_base = f\"https://api.github.com/repos/{GITHUB_USERNAME}/{REPO_NAME}\"\n",
    "    \n",
    "    try:\n",
    "        new_files_url = f\"{api_base}/contents/data/enhanced/new\"\n",
    "        response = requests.get(new_files_url)\n",
    "        \n",
    "        if response.status_code == 404:\n",
    "            return []\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"❌ GitHub API error: {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "        files = response.json()\n",
    "        enhanced_files = [f for f in files if f['name'].startswith('enhanced_') and f['name'].endswith('.json')]\n",
    "        \n",
    "        return enhanced_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking new files: {e}\")\n",
    "        return []\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 2: IMPORT WITH DEDUPLICATION\n",
    "# =============================================================================\n",
    "\n",
    "def import_enhanced_data():\n",
    "    \"\"\"FIXED: Import with matching column names\"\"\"\n",
    "    \n",
    "    print(\"\uD83D\uDE80 IMPORTING ENHANCED DATA (SCHEMA ALIGNED)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration\n",
    "    GITHUB_USERNAME = \"AlexanderHuynhKoehler\"\n",
    "    REPO_NAME = \"bluesky-data-pipeline\"\n",
    "    api_base = f\"https://api.github.com/repos/{GITHUB_USERNAME}/{REPO_NAME}\"\n",
    "    target_table = \"social_media.bluesky_enhanced_fixed_types\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Check for new files\n",
    "        new_files = check_new_files()\n",
    "        \n",
    "        if not new_files:\n",
    "            print(\"✅ No new files to process\")\n",
    "            return 0\n",
    "        \n",
    "        print(f\"\uD83D\uDCE5 Found {len(new_files)} new files\")\n",
    "        \n",
    "        # Step 2: Get existing post IDs for deduplication\n",
    "        try:\n",
    "            existing_posts_df = spark.sql(f\"\"\"\n",
    "                SELECT DISTINCT post_id \n",
    "                FROM {target_table}\n",
    "                WHERE post_id IS NOT NULL AND post_id != ''\n",
    "            \"\"\")\n",
    "            existing_post_ids = set([row['post_id'] for row in existing_posts_df.collect()])\n",
    "            print(f\"\uD83D\uDD0D {len(existing_post_ids):,} existing post IDs\")\n",
    "        except:\n",
    "            existing_post_ids = set()\n",
    "            print(\"\uD83D\uDD0D No existing posts\")\n",
    "        \n",
    "        # Step 3: Download and deduplicate\n",
    "        all_new_posts = []\n",
    "        processed_files = []\n",
    "        total_downloaded = 0\n",
    "        duplicates_filtered = 0\n",
    "        \n",
    "        for file_info in new_files:\n",
    "            file_response = requests.get(file_info['download_url'])\n",
    "            if file_response.status_code != 200:\n",
    "                continue\n",
    "            \n",
    "            enhanced_data = file_response.json()\n",
    "            posts_data = enhanced_data.get('enhanced_posts', [])\n",
    "            total_downloaded += len(posts_data)\n",
    "            \n",
    "            # Filter duplicates and add metadata\n",
    "            for post in posts_data:\n",
    "                post_id = post.get('post_id')\n",
    "                if post_id and post_id not in existing_post_ids:\n",
    "                    post['source_file'] = file_info['name']\n",
    "                    \n",
    "                    # \uD83D\uDEE0️ CRITICAL FIX: Use 'reprocessed_at' to match existing table schema\n",
    "                    post['reprocessed_at'] = datetime.now().isoformat()  # ✅ Match table column\n",
    "                    \n",
    "                    # Remove 'imported_at' if it exists to avoid conflicts\n",
    "                    if 'imported_at' in post:\n",
    "                        del post['imported_at']\n",
    "                    \n",
    "                    all_new_posts.append(post)\n",
    "                    existing_post_ids.add(post_id)\n",
    "                else:\n",
    "                    duplicates_filtered += 1\n",
    "            \n",
    "            processed_files.append(file_info['name'])\n",
    "        \n",
    "        print(f\"\uD83D\uDCCA Total downloaded: {total_downloaded}\")\n",
    "        print(f\"\uD83C\uDD95 New unique posts: {len(all_new_posts)}\")\n",
    "        print(f\"\uD83D\uDD04 Duplicates filtered: {duplicates_filtered}\")\n",
    "        \n",
    "        if not all_new_posts:\n",
    "            print(\"✅ No new unique posts\")\n",
    "            # Move files anyway\n",
    "            for filename in processed_files:\n",
    "                move_file_to_processed(filename, GITHUB_TOKEN, api_base)\n",
    "            return 0\n",
    "        \n",
    "        # Step 4: Create DataFrame with proper types\n",
    "        df = pd.DataFrame(all_new_posts)\n",
    "        \n",
    "        # Type enforcement\n",
    "        float_cols = ['ml_sentiment_score', 'emotion_confidence', 'processing_speed', 'total_engagement']\n",
    "        int_cols = ['text_length', 'text_cleaned_length', 'hashtag_count', 'mention_count']\n",
    "        string_cols = ['text', 'text_cleaned', 'author', 'ml_sentiment_label', 'dominant_emotion']\n",
    "        \n",
    "        for col in float_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0).astype('float64')\n",
    "        \n",
    "        for col in int_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "        \n",
    "        for col in string_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('').astype(str)\n",
    "        \n",
    "        print(f\"✅ DataFrame prepared: {len(df)} rows\")\n",
    "        \n",
    "        # \uD83D\uDEE0️ FIXED SCHEMA: Match your existing table EXACTLY\n",
    "        table_schema = StructType([\n",
    "            StructField(\"post_id\", StringType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"text_cleaned\", StringType(), True),\n",
    "            StructField(\"text_preview\", StringType(), True),\n",
    "            StructField(\"author\", StringType(), True),\n",
    "            StructField(\"author_display_name\", StringType(), True),\n",
    "            StructField(\"created_at\", StringType(), True),\n",
    "            StructField(\"hashtags\", StringType(), True),\n",
    "            StructField(\"mentions\", StringType(), True),\n",
    "            StructField(\"received_at\", StringType(), True),\n",
    "            StructField(\"ml_sentiment_label\", StringType(), True),\n",
    "            StructField(\"dominant_emotion\", StringType(), True),\n",
    "            StructField(\"ml_processed_at\", StringType(), True),\n",
    "            StructField(\"processed_by\", StringType(), True),\n",
    "            StructField(\"source_file\", StringType(), True),\n",
    "            StructField(\"reprocessed_at\", StringType(), True),           # ✅ MATCH TABLE\n",
    "            StructField(\"ml_sentiment_score\", DoubleType(), True),\n",
    "            StructField(\"emotion_confidence\", DoubleType(), True),\n",
    "            StructField(\"processing_speed\", DoubleType(), True),\n",
    "            StructField(\"total_engagement\", DoubleType(), True),\n",
    "            StructField(\"text_length\", LongType(), True),\n",
    "            StructField(\"text_cleaned_length\", LongType(), True),\n",
    "            StructField(\"hashtag_count\", LongType(), True),\n",
    "            StructField(\"mention_count\", LongType(), True),\n",
    "        ])\n",
    "        \n",
    "        # Select only columns that exist in both schema and data\n",
    "        schema_columns = [field.name for field in table_schema.fields]\n",
    "        available_columns = [col for col in schema_columns if col in df.columns]\n",
    "        \n",
    "        print(f\"\uD83D\uDCCB Using {len(available_columns)} matching columns\")\n",
    "        \n",
    "        # Create Spark DataFrame\n",
    "        selected_df = df[available_columns].copy()\n",
    "        spark_df = spark.createDataFrame(selected_df, schema=table_schema)\n",
    "        \n",
    "        print(\"✅ Spark DataFrame created with matching schema\")\n",
    "        \n",
    "        # Step 5: Append to table with matching schema\n",
    "        print(\"\uD83D\uDCBE Appending to Delta table...\")\n",
    "        \n",
    "        (spark_df\n",
    "            .write\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"false\")  # Schema should match exactly now\n",
    "            .saveAsTable(target_table)\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ {len(all_new_posts):,} rows appended successfully!\")\n",
    "        \n",
    "        # Step 6: Move files to processed\n",
    "        moved_count = 0\n",
    "        for filename in processed_files[:10]:  # Process first 10 files to test\n",
    "            if move_file_to_processed(filename, GITHUB_TOKEN, api_base):\n",
    "                moved_count += 1\n",
    "            else:\n",
    "                break  # Stop if file moving fails\n",
    "        \n",
    "        print(f\"✅ {moved_count} files moved to processed\")\n",
    "        \n",
    "        return len(all_new_posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Import failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 3: PIPELINE ORCHESTRATOR  \n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Main pipeline function - checks for new files and imports them\"\"\"\n",
    "    \n",
    "    print(\"\uD83C\uDFAF RUNNING COMPLETE IMPORT PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    target_table = \"social_media.bluesky_enhanced_fixed_types\"\n",
    "    \n",
    "    # Get starting row count\n",
    "    try:\n",
    "        start_count_df = spark.sql(f\"SELECT COUNT(*) as total FROM {target_table}\")\n",
    "        start_count = start_count_df.collect()[0]['total']\n",
    "        print(f\"\uD83D\uDCCA Starting table size: {start_count:,} rows\")\n",
    "    except:\n",
    "        start_count = 0\n",
    "        print(f\"\uD83D\uDCCA Table doesn't exist yet - will be created\")\n",
    "    \n",
    "    # Check for new files\n",
    "    new_files = check_new_files()\n",
    "    \n",
    "    if not new_files:\n",
    "        print(f\"✅ Pipeline complete - no new files found\")\n",
    "        print(f\"\uD83D\uDCCA Table remains: {start_count:,} rows\")\n",
    "        return {\n",
    "            'new_rows_added': 0,\n",
    "            'starting_rows': start_count,\n",
    "            'final_rows': start_count,\n",
    "            'files_processed': 0\n",
    "        }\n",
    "    \n",
    "    print(f\"\uD83D\uDCE5 Found {len(new_files)} new files:\")\n",
    "    for f in new_files:\n",
    "        print(f\"   - {f['name']}\")\n",
    "    \n",
    "    # Run import\n",
    "    new_rows_added = import_enhanced_data()\n",
    "    \n",
    "    # Get final row count\n",
    "    try:\n",
    "        final_count_df = spark.sql(f\"SELECT COUNT(*) as total FROM {target_table}\")\n",
    "        final_count = final_count_df.collect()[0]['total']\n",
    "    except:\n",
    "        final_count = start_count + new_rows_added\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"\uD83C\uDF89 PIPELINE COMPLETE\")\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"\uD83D\uDCCA DELTA TABLE CHANGES:\")\n",
    "    print(f\"   ├── Starting rows: {start_count:,}\")\n",
    "    print(f\"   ├── NEW rows added: {new_rows_added:,}  ⭐⭐⭐\")\n",
    "    print(f\"   └── Final rows: {final_count:,}\")\n",
    "    \n",
    "    if new_rows_added > 0:\n",
    "        growth_pct = (new_rows_added / start_count * 100) if start_count > 0 else 0\n",
    "        print(f\"\uD83D\uDCC8 Table growth: +{growth_pct:.1f}%\")\n",
    "    \n",
    "    print(f\"✅ Files processed: {len(new_files)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'new_rows_added': new_rows_added,\n",
    "        'starting_rows': start_count,\n",
    "        'final_rows': final_count,\n",
    "        'files_processed': len(new_files)\n",
    "    }\n",
    "\n",
    "def move_file_to_processed(filename, github_token, api_base):\n",
    "    \"\"\"Helper: Move file from new to processed folder\"\"\"\n",
    "    try:\n",
    "        headers = {\"Authorization\": f\"token {github_token}\"}\n",
    "        \n",
    "        new_file_url = f\"{api_base}/contents/data/enhanced/new/{filename}\"\n",
    "        response = requests.get(new_file_url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return False\n",
    "        \n",
    "        file_data = response.json()\n",
    "        \n",
    "        processed_file_url = f\"{api_base}/contents/data/enhanced/processed/{filename}\"\n",
    "        upload_payload = {\n",
    "            \"message\": f\"Import complete: {filename}\",\n",
    "            \"content\": file_data['content'],\n",
    "            \"branch\": \"main\"\n",
    "        }\n",
    "        \n",
    "        upload_response = requests.put(processed_file_url, headers=headers, json=upload_payload)\n",
    "        \n",
    "        if upload_response.status_code in [200, 201]:\n",
    "            delete_payload = {\n",
    "                \"message\": f\"Move to processed: {filename}\",\n",
    "                \"sha\": file_data['sha'],\n",
    "                \"branch\": \"main\"\n",
    "            }\n",
    "            requests.delete(new_file_url, headers=headers, json=delete_payload)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# SIMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29e25281-0ef5-42a5-bfb1-78c5e4f52ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_customers_silver.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
