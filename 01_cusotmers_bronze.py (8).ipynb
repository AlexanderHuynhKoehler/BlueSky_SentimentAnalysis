{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268384e5-5f25-4649-8ec4-f65451f2ee39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Setting up simplified raw data collection pipeline...\n✅ Simplified raw data pipeline started!\n\uD83C\uDD94 Query ID: c55441f4-b701-440a-8a60-e040814d5b2f\n\uD83D\uDCC1 Raw data location: /mnt/delta/tables/bluesky_raw_posts\n\uD83C\uDFAF Collecting raw data for college cluster ML processing\n\uD83D\uDCCA Raw posts table created: social_media.bluesky_raw_posts\n\uD83D\uDCC8 Processing: 0.0 input/sec, 68.87052341597796 processed/sec\n⚠️ No input data - check Azure Function\n\uD83D\uDCCA Raw posts table now has: 9375 posts\n\uD83C\uDF89 SUCCESS: Raw data pipeline working!\n\uD83D\uDE80 Ready for college cluster connection test\n\n\uD83D\uDCCB Sample raw posts:\n+-------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------+\n|post_id      |author                           |text                                                                                                                                                                                 |hashtags                                                                                   |received_at            |\n+-------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------+\n|3lvybvdi5hp2t|oppermanreport.bsky.social       |\uD83D\uDCE3 New Podcast! \"Mr Trump Pal Alex Jones Apologises to James Alefantis Over Pizzagate 1\" on @Spreaker #alefantis #alex #apologises #james #jones #mr #over #pal #pizzagate #to #trump|[#mr, #to, #trump, #pizzagate, #jones, #apologises, #over, #pal, #alefantis, #james, #alex]|2025-08-10 13:35:19.897|\n|3lvykbazqct2n|teep47.bsky.social               |#trump\\n\\nis he dead yet ??                                                                                                                                                          |[#trump]                                                                                   |2025-08-10 13:35:19.897|\n|3lvyatjj5cc2b|blueginger99.bsky.social         |#GOPcorruption #GOP #trump #bribery #apple #timcook                                                                                                                                  |[#GOPcorruption, #trump, #timcook, #bribery, #GOP, #apple]                                 |2025-08-10 13:35:19.897|\n|3lvyjboyy3f52|twitter.twitter.social.ap.brid.gy|No #Trump is a good Trump.                                                                                                                                                           |[#Trump]                                                                                   |2025-08-10 13:35:19.897|\n|3lvyb3y437n2n|toppnews.bsky.social             |\\n#Trump   #Volodymyr Zelenskyy   #Zelenskyy                                                                                                                                         |[#Zelenskyy, #Volodymyr, #Trump]                                                           |2025-08-10 13:35:19.897|\n+-------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------+\n\n\n============================================================\n\uD83C\uDFAF DATABRICKS SIMPLIFIED PIPELINE COMPLETE!\nNext: Test college cluster connection to this table\n============================================================\n"
     ]
    }
   ],
   "source": [
    "# DATABRICKS: Complete Simplified Pipeline (No ML Processing)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(\"\uD83D\uDE80 Setting up simplified raw data collection pipeline...\")\n",
    "\n",
    "\n",
    "ehConf = {\n",
    "    'eventhubs.connectionString': sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(CONNECTION_STRING),\n",
    "    'eventhubs.consumerGroup': \"$Default\",\n",
    "    'eventhubs.startingPosition': json.dumps({\n",
    "        \"offset\": \"-1\",\n",
    "        \"seqNo\": -1,\n",
    "        \"enqueuedTime\": None,\n",
    "        \"isInclusive\": False\n",
    "    })\n",
    "}\n",
    "\n",
    "# JSON Schema (keep unchanged)\n",
    "json_schema = StructType([\n",
    "    StructField(\"post_id\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"text_preview\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"author_display_name\", StringType(), True),\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"hashtags\", ArrayType(StringType()), True),\n",
    "    StructField(\"mentions\", ArrayType(StringType()), True),\n",
    "    StructField(\"engagement\", StructType([\n",
    "        StructField(\"likes\", IntegerType(), True),\n",
    "        StructField(\"reposts\", IntegerType(), True),\n",
    "        StructField(\"replies\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"collection_metadata\", StructType([\n",
    "        StructField(\"collected_at\", StringType(), True),\n",
    "        StructField(\"collector_version\", StringType(), True),\n",
    "        StructField(\"source\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "def deep_clean_text(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    cleaned = re.sub(r'#\\w+', '', text)\n",
    "    cleaned = re.sub(r'http\\S+|www.\\S+', '', cleaned)\n",
    "    cleaned = re.sub(r'@\\w+', '', cleaned)\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned.strip()\n",
    "\n",
    "deep_clean_udf = udf(deep_clean_text, StringType())\n",
    "\n",
    "raw_stream_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"eventhubs\")\n",
    "    .options(**ehConf)\n",
    "    .load()\n",
    "    \n",
    "    # Parse JSON\n",
    "    .select(\n",
    "        from_json(col(\"body\").cast(\"string\"), json_schema).alias(\"data\"),\n",
    "        col(\"enqueuedTime\").alias(\"eventhub_enqueued_time\"),\n",
    "        col(\"offset\").alias(\"eventhub_offset\"),\n",
    "        col(\"partition\").alias(\"eventhub_partition\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"data.*\"),\n",
    "        col(\"eventhub_enqueued_time\"),\n",
    "        col(\"eventhub_offset\"),\n",
    "        col(\"eventhub_partition\")\n",
    "    )\n",
    "    \n",
    "    # Basic cleaning and filtering (NO ML)\n",
    "    .withColumn(\"text_cleaned\", deep_clean_udf(col(\"text\")))\n",
    "    .withColumn(\"text_preview_cleaned\", deep_clean_udf(col(\"text_preview\")))\n",
    "    .filter(col(\"text\").isNotNull())\n",
    "    .filter(length(col(\"text\")) >= 20)  # Keep decent posts\n",
    "    .filter(col(\"author\").isNotNull())\n",
    "    \n",
    "    # Add derived columns (NO SENTIMENT ANALYSIS)\n",
    "    .withColumn(\"received_at\", current_timestamp())\n",
    "    .withColumn(\"text_length\", length(col(\"text\")))\n",
    "    .withColumn(\"text_cleaned_length\", length(col(\"text_cleaned\")))\n",
    "    .withColumn(\"hashtag_count\", size(col(\"hashtags\")))\n",
    "    .withColumn(\"mention_count\", size(col(\"mentions\")))\n",
    "    .withColumn(\"total_engagement\",\n",
    "        coalesce(col(\"engagement.likes\"), lit(0)) +\n",
    "        coalesce(col(\"engagement.replies\"), lit(0)) * 2 +\n",
    "        coalesce(col(\"engagement.reposts\"), lit(0)) * 1.5\n",
    "    )\n",
    "    \n",
    "    .dropDuplicates([\"post_id\"])\n",
    "    \n",
    "    .select(\n",
    "        col(\"post_id\"),\n",
    "        col(\"text\"),                    # Original text with hashtags\n",
    "        col(\"text_cleaned\"),            # Cleaned text for ML\n",
    "        col(\"text_preview\"),\n",
    "        col(\"author\"),\n",
    "        col(\"author_display_name\"),\n",
    "        col(\"created_at\"),\n",
    "        col(\"hashtags\"),\n",
    "        col(\"mentions\"),\n",
    "        col(\"engagement\"),\n",
    "        col(\"collection_metadata\"),\n",
    "        col(\"received_at\"),\n",
    "        col(\"text_length\"),\n",
    "        col(\"text_cleaned_length\"),\n",
    "        col(\"hashtag_count\"),\n",
    "        col(\"mention_count\"),\n",
    "        col(\"total_engagement\"),\n",
    "        col(\"eventhub_enqueued_time\"),\n",
    "        col(\"eventhub_offset\"),\n",
    "        col(\"eventhub_partition\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# New paths for raw data\n",
    "RAW_CHECKPOINT_PATH = \"/mnt/delta/checkpoints/bluesky_raw_posts\"\n",
    "RAW_DELTA_PATH = \"/mnt/delta/tables/bluesky_raw_posts\"\n",
    "\n",
    "# Start streaming query (should be fast without ML)\n",
    "raw_query = (\n",
    "    raw_stream_df\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", RAW_CHECKPOINT_PATH)\n",
    "    .option(\"path\", RAW_DELTA_PATH)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"✅ Simplified raw data pipeline started!\")\n",
    "print(f\"\uD83C\uDD94 Query ID: {raw_query.id}\")\n",
    "print(f\"\uD83D\uDCC1 Raw data location: {RAW_DELTA_PATH}\")\n",
    "print(\"\uD83C\uDFAF Collecting raw data for college cluster ML processing\")\n",
    "\n",
    "# Wait a moment then create table\n",
    "import time\n",
    "time.sleep(30)\n",
    "\n",
    "# Create database and table\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS social_media\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS social_media.bluesky_raw_posts\n",
    "    USING DELTA\n",
    "    LOCATION '{RAW_DELTA_PATH}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"\uD83D\uDCCA Raw posts table created: social_media.bluesky_raw_posts\")\n",
    "\n",
    "# Check pipeline status\n",
    "time.sleep(30)\n",
    "if raw_query.lastProgress:\n",
    "    input_rate = raw_query.lastProgress.get('inputRowsPerSecond', 0)\n",
    "    processed_rate = raw_query.lastProgress.get('processedRowsPerSecond', 0)\n",
    "    print(f\"\uD83D\uDCC8 Processing: {input_rate} input/sec, {processed_rate} processed/sec\")\n",
    "    \n",
    "    if input_rate > 0:\n",
    "        print(\"✅ Event Hubs data flowing successfully!\")\n",
    "    else:\n",
    "        print(\"⚠️ No input data - check Azure Function\")\n",
    "\n",
    "try:\n",
    "    count_query = spark.sql(\"SELECT COUNT(*) as count FROM social_media.bluesky_raw_posts\")\n",
    "    row_count = count_query.collect()[0]['count']\n",
    "    print(f\"\uD83D\uDCCA Raw posts table now has: {row_count} posts\")\n",
    "    \n",
    "    if row_count > 0:\n",
    "        print(\"\uD83C\uDF89 SUCCESS: Raw data pipeline working!\")\n",
    "        print(\"\uD83D\uDE80 Ready for college cluster connection test\")\n",
    "        \n",
    "        # Show sample data\n",
    "        sample = spark.sql(\"\"\"\n",
    "            SELECT post_id, author, text, hashtags, received_at\n",
    "            FROM social_media.bluesky_raw_posts\n",
    "            ORDER BY received_at DESC\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        print(\"\\n\uD83D\uDCCB Sample raw posts:\")\n",
    "        sample.show(truncate=False)\n",
    "    else:\n",
    "        print(\"⏳ Waiting for data to flow...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Table check failed: {e}\")\n",
    "    print(\"Table may still be initializing...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\uD83C\uDFAF DATABRICKS SIMPLIFIED PIPELINE COMPLETE!\")\n",
    "print(\"Next: Test college cluster connection to this table\")\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "from datetime import datetime, timedelta  # <-- This import is missing\n",
    "import time\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd05575-9c5e-417a-b470-66f857371c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCB Current table schema:\n+--------------------+--------------------+-------+\n|            col_name|           data_type|comment|\n+--------------------+--------------------+-------+\n|             post_id|              string|   NULL|\n|                text|              string|   NULL|\n|        text_cleaned|              string|   NULL|\n|        text_preview|              string|   NULL|\n|              author|              string|   NULL|\n| author_display_name|              string|   NULL|\n|          created_at|              string|   NULL|\n|            hashtags|       array<string>|   NULL|\n|            mentions|       array<string>|   NULL|\n|          engagement|struct<likes:int,...|   NULL|\n| collection_metadata|struct<collected_...|   NULL|\n|         received_at|           timestamp|   NULL|\n|         text_length|                 int|   NULL|\n| text_cleaned_length|                 int|   NULL|\n|       hashtag_count|                 int|   NULL|\n|       mention_count|                 int|   NULL|\n|    total_engagement|              double|   NULL|\n|eventhub_enqueued...|           timestamp|   NULL|\n|     eventhub_offset|              string|   NULL|\n|  eventhub_partition|              string|   NULL|\n+--------------------+--------------------+-------+\nonly showing top 20 rows\n\n✅ exported_to_github column already exists!\n⏭️ Skipping default value setup (not needed)\n\uD83D\uDD04 Initializing all existing posts as FALSE...\n\uD83D\uDD59 Exporting 10-minute batch from Databricks...\n\uD83D\uDCCA Querying last 10 minutes of data...\n\uD83D\uDCE4 Found 35 posts from last 10 minutes\n✅ Batch uploaded: data/incremental/batch_20250810_140036.json\n✅ Processing manifest updated\n✅ Successfully exported 10-minute batch: 35 posts\n✅ Automated export at 2025-08-10 14:00:38.081299\n✅ All existing posts marked as not exported!\n\uD83D\uDCCB Final table schema:\n+--------------------+--------------------+-------+\n|            col_name|           data_type|comment|\n+--------------------+--------------------+-------+\n|             post_id|              string|   NULL|\n|                text|              string|   NULL|\n|        text_cleaned|              string|   NULL|\n|        text_preview|              string|   NULL|\n|              author|              string|   NULL|\n| author_display_name|              string|   NULL|\n|          created_at|              string|   NULL|\n|            hashtags|       array<string>|   NULL|\n|            mentions|       array<string>|   NULL|\n|          engagement|struct<likes:int,...|   NULL|\n| collection_metadata|struct<collected_...|   NULL|\n|         received_at|           timestamp|   NULL|\n|         text_length|                 int|   NULL|\n| text_cleaned_length|                 int|   NULL|\n|       hashtag_count|                 int|   NULL|\n|       mention_count|                 int|   NULL|\n|    total_engagement|              double|   NULL|\n|eventhub_enqueued...|           timestamp|   NULL|\n|     eventhub_offset|              string|   NULL|\n|  eventhub_partition|              string|   NULL|\n+--------------------+--------------------+-------+\nonly showing top 20 rows\n\n\uD83D\uDCCA Progress: 0/11244 posts exported (0.0%)\n\uD83D\uDCCB Remaining: 11244 posts\n⏱️ Est. 225 more batches needed\n\uD83E\uDDEA Testing single batch export...\n\uD83D\uDD04 Looking for next 50 unmarked posts...\n\uD83D\uDCE4 Found 50 unmarked posts to export\n✅ Uploaded: data/incremental/batch_20250810_140056.json\n✅ 50 posts marked as exported\n✅ Batch exported: 50 posts\n\uD83C\uDFF7️ Marked 50 posts as exported\n\uD83D\uDCCA Progress: 50/11244 posts exported (0.4%)\n\uD83D\uDCCB Remaining: 11194 posts\n⏱️ Est. 224 more batches needed\n\uD83C\uDF89 Single batch test successful!\n\n\uD83E\uDD16 Ready for automated 10-minute exports:\nautomated_batch_export()  # Uncomment to start automation\n"
     ]
    }
   ],
   "source": [
    "class SimpleBatchExporter:\n",
    "    \n",
    "    def __init__(self, github_username, repo_name, personal_token):\n",
    "        self.username = github_username\n",
    "        self.repo_name = repo_name\n",
    "        self.token = personal_token\n",
    "        self.api_base = f\"https://api.github.com/repos/{github_username}/{repo_name}\"\n",
    "        self.batch_size = 50\n",
    "        \n",
    "    def setup_export_column_once(self):\n",
    "        \"\"\"One-time setup: Add export tracking column (Delta Lake compatible)\"\"\"\n",
    "        try:\n",
    "            print(\"\uD83D\uDCCB Current table schema:\")\n",
    "            current_schema = spark.sql(\"DESCRIBE social_media.bluesky_raw_posts\")\n",
    "            current_schema.show()\n",
    "            \n",
    "            # Check if column already exists\n",
    "            columns = [row.col_name for row in current_schema.collect()]\n",
    "            if 'exported_to_github' in columns:\n",
    "                print(\"✅ exported_to_github column already exists!\")\n",
    "            else:\n",
    "                print(\"\uD83D\uDD27 Step 1: Adding exported_to_github column...\")\n",
    "                spark.sql(\"\"\"\n",
    "                    ALTER TABLE social_media.bluesky_raw_posts \n",
    "                    ADD COLUMN exported_to_github BOOLEAN\n",
    "                \"\"\")\n",
    "                print(\"✅ Column added!\")\n",
    "            \n",
    "            print(\"⏭️ Skipping default value setup (not needed)\")\n",
    "            \n",
    "            # Step 2: Initialize all existing posts as not exported\n",
    "            print(\"\uD83D\uDD04 Initializing all existing posts as FALSE...\")\n",
    "            spark.sql(\"\"\"\n",
    "                UPDATE social_media.bluesky_raw_posts \n",
    "                SET exported_to_github = FALSE \n",
    "                WHERE exported_to_github IS NULL\n",
    "            \"\"\")\n",
    "            \n",
    "            print(\"✅ All existing posts marked as not exported!\")\n",
    "            \n",
    "            # Verify final schema\n",
    "            print(\"\uD83D\uDCCB Final table schema:\")\n",
    "            updated_schema = spark.sql(\"DESCRIBE social_media.bluesky_raw_posts\")\n",
    "            updated_schema.show()\n",
    "            \n",
    "            # Show counts\n",
    "            self.show_progress()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Column setup error: {e}\")\n",
    "            print(\"\uD83D\uDD0D Error details:\", str(e))\n",
    "    \n",
    "    def export_next_batch(self):\n",
    "        \"\"\"Export next 50 unmarked posts (or whatever's available)\"\"\"\n",
    "        \n",
    "        print(f\"\uD83D\uDD04 Looking for next {self.batch_size} unmarked posts...\")\n",
    "        \n",
    "        # Dead simple query: oldest unmarked posts first\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            post_id, text, text_cleaned, text_preview, author, author_display_name,\n",
    "            created_at, hashtags, mentions, engagement, collection_metadata,\n",
    "            received_at, text_length, text_cleaned_length, hashtag_count,\n",
    "            mention_count, total_engagement\n",
    "        FROM social_media.bluesky_raw_posts\n",
    "        WHERE exported_to_github = FALSE OR exported_to_github IS NULL\n",
    "        ORDER BY received_at ASC\n",
    "        LIMIT {self.batch_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            df = spark.sql(query).toPandas()\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(\"\uD83C\uDF89 All posts exported! Nothing left to process.\")\n",
    "                return False\n",
    "                \n",
    "            print(f\"\uD83D\uDCE4 Found {len(df)} unmarked posts to export\")\n",
    "            \n",
    "            # Get post IDs for marking later\n",
    "            post_ids = df['post_id'].tolist()\n",
    "            \n",
    "            # Create batch\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            batch_data = {\n",
    "                'batch_info': {\n",
    "                    'timestamp': timestamp,\n",
    "                    'post_count': len(df),\n",
    "                    'batch_size_requested': self.batch_size,\n",
    "                    'batch_size_actual': len(df),\n",
    "                    'export_method': 'simple_batch_oldest_first',\n",
    "                    'posts_remaining_estimate': self.estimate_remaining_posts()\n",
    "                },\n",
    "                'posts': df.to_dict('records')\n",
    "            }\n",
    "            \n",
    "            # Upload to GitHub\n",
    "            success = self.upload_to_github(batch_data, timestamp)\n",
    "            \n",
    "            if success:\n",
    "                # Mark these posts as exported\n",
    "                self.mark_posts_exported(post_ids)\n",
    "                print(f\"✅ Batch exported: {len(df)} posts\")\n",
    "                print(f\"\uD83C\uDFF7️ Marked {len(post_ids)} posts as exported\")\n",
    "                \n",
    "                # Show progress\n",
    "                self.show_progress()\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌ Upload failed - posts remain unmarked\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Export batch failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def mark_posts_exported(self, post_ids):\n",
    "        \"\"\"Mark specific posts as exported\"\"\"\n",
    "        try:\n",
    "            # Convert to SQL format\n",
    "            id_list = \"', '\".join(post_ids)\n",
    "            \n",
    "            spark.sql(f\"\"\"\n",
    "                UPDATE social_media.bluesky_raw_posts \n",
    "                SET exported_to_github = TRUE \n",
    "                WHERE post_id IN ('{id_list}')\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"✅ {len(post_ids)} posts marked as exported\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to mark posts: {e}\")\n",
    "    \n",
    "    def estimate_remaining_posts(self):\n",
    "        \"\"\"Quick count of remaining posts\"\"\"\n",
    "        try:\n",
    "            result = spark.sql(\"\"\"\n",
    "                SELECT COUNT(*) as remaining \n",
    "                FROM social_media.bluesky_raw_posts \n",
    "                WHERE exported_to_github = FALSE OR exported_to_github IS NULL\n",
    "            \"\"\").collect()\n",
    "            \n",
    "            return result[0]['remaining']\n",
    "            \n",
    "        except:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    def show_progress(self):\n",
    "        \"\"\"Show current export progress\"\"\"\n",
    "        try:\n",
    "            progress = spark.sql(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_posts,\n",
    "                    COUNT(CASE WHEN exported_to_github = TRUE THEN 1 END) as exported,\n",
    "                    COUNT(CASE WHEN exported_to_github = FALSE OR exported_to_github IS NULL THEN 1 END) as remaining,\n",
    "                    ROUND(COUNT(CASE WHEN exported_to_github = TRUE THEN 1 END) * 100.0 / COUNT(*), 1) as percent_done\n",
    "                FROM social_media.bluesky_raw_posts\n",
    "            \"\"\").collect()[0]\n",
    "            \n",
    "            print(f\"\uD83D\uDCCA Progress: {progress['exported']}/{progress['total_posts']} posts exported ({progress['percent_done']}%)\")\n",
    "            print(f\"\uD83D\uDCCB Remaining: {progress['remaining']} posts\")\n",
    "            \n",
    "            if progress['remaining'] > 0:\n",
    "                batches_remaining = (progress['remaining'] + self.batch_size - 1) // self.batch_size\n",
    "                print(f\"⏱️ Est. {batches_remaining} more batches needed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Progress check failed: {e}\")\n",
    "    \n",
    "    def upload_to_github(self, batch_data, timestamp):\n",
    "        \"\"\"Upload batch to GitHub\"\"\"\n",
    "        try:\n",
    "            import base64\n",
    "            \n",
    "            json_content = json.dumps(batch_data, indent=2, default=str)\n",
    "            encoded_content = base64.b64encode(json_content.encode()).decode()\n",
    "            \n",
    "            filename = f\"data/incremental/batch_{timestamp}.json\"\n",
    "            \n",
    "            headers = {\n",
    "                \"Authorization\": f\"token {self.token}\",\n",
    "                \"Accept\": \"application/vnd.github.v3+json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"message\": f\"Simple batch: {batch_data['batch_info']['post_count']} posts ({timestamp})\",\n",
    "                \"content\": encoded_content,\n",
    "                \"branch\": \"main\"\n",
    "            }\n",
    "            \n",
    "            upload_url = f\"{self.api_base}/contents/{filename}\"\n",
    "            response = requests.put(upload_url, headers=headers, json=payload)\n",
    "            \n",
    "            if response.status_code in [200, 201]:\n",
    "                print(f\"✅ Uploaded: {filename}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"❌ Upload failed: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Upload error: {e}\")\n",
    "            return False\n",
    "\n",
    "# =============================================================================\n",
    "# AUTOMATED 10-MINUTE BATCH PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def automated_batch_export():\n",
    "    \"\"\"Run batch exports every 10 minutes\"\"\"\n",
    "    \n",
    "    exporter = SimpleBatchExporter(GITHUB_USERNAME, REPO_NAME, GITHUB_TOKEN)\n",
    "    \n",
    "    # One-time setup\n",
    "    exporter.setup_export_column_once()\n",
    "    \n",
    "    print(\"\uD83D\uDE80 Starting automated 10-minute batch exports...\")\n",
    "    print(f\"\uD83D\uDCE6 Batch size: {exporter.batch_size} posts per export\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"⏰ Batch export at {datetime.now()}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            success = exporter.export_next_batch()\n",
    "            \n",
    "            if success:\n",
    "                print(\"✅ Batch export completed successfully\")\n",
    "            else:\n",
    "                print(\"\uD83D\uDCED No posts to export - all caught up!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Batch export error: {e}\")\n",
    "        \n",
    "        print(f\"⏸️ Waiting 10 minutes until next batch...\")\n",
    "        time.sleep(600)  # 10 minutes\n",
    "\n",
    "# Configuration\n",
    "GITHUB_USERNAME = \"AlexanderHuynhKoehler\" \n",
    "REPO_NAME = \"bluesky-data-pipeline\"\n",
    "GITHUB_TOKEN = \"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\"\n",
    "\n",
    "exporter = SimpleBatchExporter(GITHUB_USERNAME, REPO_NAME, GITHUB_TOKEN)\n",
    "\n",
    "\n",
    "print(\"\uD83E\uDDEA Testing single batch export...\")\n",
    "success = exporter.export_next_batch()\n",
    "\n",
    "if success:\n",
    "    print(\"\uD83C\uDF89 Single batch test successful!\")\n",
    "    print(\"\\n\uD83E\uDD16 Ready for automated 10-minute exports:\")\n",
    "else:\n",
    "    print(\"❌ Test failed or no posts to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81a72ec-b9d0-49f3-aa7e-ef0d960f0102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The cluster was unhealthy or has been terminated. Please restart the cluster or attach this notebook to a different cluster.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "automated_batch_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb8649f-e8fe-48ec-9da4-87f42756d285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_cusotmers_bronze.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
