{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735fb1c6-5122-4265-8959-0b1cc4498a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "print(\"üîç ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "print(f\"\\nCPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"GPU: Not available (CPU only)\")\n",
    "\n",
    "print(f\"\\nLibrary Versions:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Transformers: {transformers.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "\n",
    "print(f\"\\nüß† Testing RoBERTa Model Loading...\")\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    classifier = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    result = classifier(\"This is a test sentence for sentiment analysis!\")\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ RoBERTa loaded successfully in {load_time:.2f} seconds\")\n",
    "    print(f\"   Test result: {result}\")\n",
    "    \n",
    "    test_texts = [\"I love this!\"] * 100\n",
    "    start_time = time.time()\n",
    "    results = classifier(test_texts)\n",
    "    batch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Batch processing: 100 texts in {batch_time:.2f} seconds\")\n",
    "    print(f\"   Speed: {100/batch_time:.1f} texts/second\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå RoBERTa loading failed: {e}\")\n",
    "\n",
    "print(f\"\\nüîó Testing Azure Event Hubs connectivity...\")\n",
    "try:\n",
    "    # Just test import and client creation (don't actually connect)\n",
    "    from azure.eventhub import EventHubConsumerClient\n",
    "    print(\"‚úÖ Azure Event Hubs library available\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Azure Event Hubs failed: {e}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"üéØ Environment verification complete!\")\n",
    "\n",
    "# Resource recommendations\n",
    "total_ram = psutil.virtual_memory().total / (1024**3)\n",
    "if total_ram < 16:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Less than 16GB RAM - may struggle with large models\")\n",
    "elif total_ram < 32:\n",
    "    print(\"‚úÖ RAM sufficient for basic processing\")\n",
    "else:\n",
    "    print(\"üöÄ Excellent RAM for heavy processing\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    if gpu_memory < 8:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Less than 8GB GPU memory - may need CPU processing\")\n",
    "    else:\n",
    "        print(\"üéÆ Excellent GPU setup for ML models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377413c-971f-482a-b00f-edd2337ba03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE FIXED COLLEGE CLUSTER PIPELINE\n",
    "# All logical issues resolved - ready to run\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "# =============================================================================\n",
    "# CLASS 1: GitHub Data Manager\n",
    "# =============================================================================\n",
    "\n",
    "class UpdatedGitHubDataManager:\n",
    "    \"\"\"Updated GitHub manager with FIXED batch processing logic - NOW PROCESSES ALL BATCHES\"\"\"\n",
    "    \n",
    "    def __init__(self, username, repo_name):\n",
    "        self.username = username\n",
    "        self.repo_name = repo_name\n",
    "        self.repo_url = f\"https://github.com/{username}/{repo_name}.git\"\n",
    "        self.repo_dir = f\"./{repo_name}/\"\n",
    "        self.processed_batches = set()  # Track processed batches\n",
    "        self.setup_repo()\n",
    "        \n",
    "    def setup_repo(self):\n",
    "        \"\"\"Clone and sync repo\"\"\"\n",
    "        print(\"üìÅ Setting up GitHub repository...\")\n",
    "        \n",
    "        if not os.path.exists(self.repo_dir):\n",
    "            print(\"üì• Cloning repository...\")\n",
    "            result = subprocess.run(['git', 'clone', self.repo_url], \n",
    "                                  capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"‚úÖ Repository cloned\")\n",
    "            else:\n",
    "                print(f\"‚ùå Clone failed: {result.stderr}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚úÖ Repository exists\")\n",
    "        \n",
    "        self.force_sync()\n",
    "        self.load_processed_batches()\n",
    "        return True\n",
    "    \n",
    "    def force_sync(self):\n",
    "        \"\"\"Force sync with GitHub\"\"\"\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(self.repo_dir)\n",
    "        \n",
    "        try:\n",
    "            subprocess.run(['git', 'fetch', 'origin'], check=True)\n",
    "            subprocess.run(['git', 'reset', '--hard', 'origin/main'], check=True)\n",
    "            print(\"‚úÖ Synced with GitHub\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Sync warning: {e}\")\n",
    "        finally:\n",
    "            os.chdir(original_dir)\n",
    "    \n",
    "    def load_processed_batches(self):\n",
    "        \"\"\"Load list of already processed batches\"\"\"\n",
    "        batch_history_file = f\"{self.repo_dir}/processed_batches.json\"\n",
    "        \n",
    "        if os.path.exists(batch_history_file):\n",
    "            with open(batch_history_file, 'r') as f:\n",
    "                history = json.load(f)\n",
    "                self.processed_batches = set(history.get('processed_batches', []))\n",
    "                print(f\"üìã Batch history: {len(self.processed_batches)} batches already processed\")\n",
    "        else:\n",
    "            print(\"üìã No batch history - starting fresh\")\n",
    "    \n",
    "    def find_all_unprocessed_batches(self):\n",
    "        \"\"\"‚úÖ NEW: Find ALL unprocessed batches instead of just one\"\"\"\n",
    "        print(\"üîç Finding ALL unprocessed batches...\")\n",
    "        \n",
    "        incremental_dir = f\"{self.repo_dir}/data/incremental/\"\n",
    "        \n",
    "        if not os.path.exists(incremental_dir):\n",
    "            print(\"‚ùå No incremental directory - using fallback method\")\n",
    "            return []\n",
    "        \n",
    "        # Get all unprocessed batch files\n",
    "        unprocessed_batches = []\n",
    "        for f in os.listdir(incremental_dir):\n",
    "            if f.endswith('.json') and f.startswith('batch_'):\n",
    "                if f not in self.processed_batches:  # Only unprocessed batches\n",
    "                    filepath = os.path.join(incremental_dir, f)\n",
    "                    mtime = os.path.getmtime(filepath)\n",
    "                    unprocessed_batches.append((f, mtime, filepath))\n",
    "        \n",
    "        # Sort by timestamp (oldest first - process in order)\n",
    "        unprocessed_batches.sort(key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"üì¶ Found {len(unprocessed_batches)} unprocessed batches:\")\n",
    "        for filename, mtime, _ in unprocessed_batches:\n",
    "            print(f\"   - {filename} (modified: {datetime.fromtimestamp(mtime)})\")\n",
    "        \n",
    "        return unprocessed_batches\n",
    "    \n",
    "    def find_unprocessed_incremental_batch(self):\n",
    "        \"\"\"LEGACY: Keep for compatibility - now returns first unprocessed batch\"\"\"\n",
    "        unprocessed_batches = self.find_all_unprocessed_batches()\n",
    "        \n",
    "        if not unprocessed_batches:\n",
    "            print(\"üìã No unprocessed incremental batches found\")\n",
    "            return self.find_latest_data_file_fallback()\n",
    "        \n",
    "        # Return first (oldest) unprocessed batch\n",
    "        filename, mtime, filepath = unprocessed_batches[0]\n",
    "        print(f\"üì¶ Next unprocessed batch: {filename}\")\n",
    "        return filepath, filename\n",
    "    \n",
    "    def find_latest_data_file_fallback(self):\n",
    "        \"\"\"Fallback to old method for compatibility\"\"\"\n",
    "        print(\"üîÑ Using fallback method (raw data folder)...\")\n",
    "        \n",
    "        data_dir = f\"{self.repo_dir}/data/raw/\"\n",
    "        \n",
    "        if not os.path.exists(data_dir):\n",
    "            print(\"‚ùå No data directories found\")\n",
    "            return None, None\n",
    "        \n",
    "        files = []\n",
    "        for f in os.listdir(data_dir):\n",
    "            if f.endswith(('.parquet', '.json')):\n",
    "                filepath = os.path.join(data_dir, f)\n",
    "                mtime = os.path.getmtime(filepath)\n",
    "                files.append((f, mtime, filepath))\n",
    "        \n",
    "        if not files:\n",
    "            print(\"‚ùå No data files found\")\n",
    "            return None, None\n",
    "        \n",
    "        latest_file = max(files, key=lambda x: x[1])\n",
    "        filename, mtime, filepath = latest_file\n",
    "        \n",
    "        print(f\"üìÅ Latest file (fallback): {filename}\")\n",
    "        return filepath, filename\n",
    "    \n",
    "    def load_data_file(self, filepath):\n",
    "        \"\"\"Load data file (handles both formats)\"\"\"\n",
    "        try:\n",
    "            if filepath.endswith('.parquet'):\n",
    "                df = pd.read_parquet(filepath)\n",
    "                print(f\"‚úÖ Loaded parquet: {len(df)} posts\")\n",
    "            elif filepath.endswith('.json'):\n",
    "                with open(filepath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Handle incremental batch format\n",
    "                if 'posts' in data:\n",
    "                    df = pd.DataFrame(data['posts'])\n",
    "                    print(f\"‚úÖ Loaded incremental batch: {len(df)} posts\")\n",
    "                elif 'batch_info' in data:\n",
    "                    df = pd.DataFrame(data['posts'])\n",
    "                    print(f\"‚úÖ Loaded 10-minute batch: {len(df)} posts\")\n",
    "                else:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    print(f\"‚úÖ Loaded JSON: {len(df)} posts\")\n",
    "            else:\n",
    "                print(f\"‚ùå Unknown file format: {filepath}\")\n",
    "                return None\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {filepath}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_latest_data(self):\n",
    "        \"\"\"LEGACY: Get single batch (keep for compatibility)\"\"\"\n",
    "        self.force_sync()\n",
    "        \n",
    "        # Try incremental batch first, then fallback\n",
    "        file_info = self.find_unprocessed_incremental_batch()\n",
    "        if not file_info[0]:\n",
    "            return None, None\n",
    "        \n",
    "        filepath, filename = file_info\n",
    "        df = self.load_data_file(filepath)\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"üéØ Ready to process {len(df)} posts from {filename}\")\n",
    "            return df, filename\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    def get_all_unprocessed_data(self):\n",
    "        \"\"\"‚úÖ NEW: Get ALL unprocessed batches as list of (df, filename) tuples\"\"\"\n",
    "        print(\"üöÄ GETTING ALL UNPROCESSED DATA\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        self.force_sync()\n",
    "        \n",
    "        unprocessed_batches = self.find_all_unprocessed_batches()\n",
    "        \n",
    "        if not unprocessed_batches:\n",
    "            print(\"‚úÖ No unprocessed batches found\")\n",
    "            return []\n",
    "        \n",
    "        batch_data_list = []\n",
    "        \n",
    "        for filename, mtime, filepath in unprocessed_batches:\n",
    "            df = self.load_data_file(filepath)\n",
    "            \n",
    "            if df is not None and len(df) > 0:\n",
    "                batch_data_list.append((df, filename))\n",
    "                print(f\"   ‚úÖ Added {filename}: {len(df)} posts\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Skipped {filename}: no valid data\")\n",
    "        \n",
    "        total_posts = sum(len(df) for df, _ in batch_data_list)\n",
    "        print(f\"\\nüìä READY TO PROCESS:\")\n",
    "        print(f\"   Batches: {len(batch_data_list)}\")\n",
    "        print(f\"   Total posts: {total_posts}\")\n",
    "        \n",
    "        return batch_data_list\n",
    "    \n",
    "    def mark_batch_processed(self, batch_filename):\n",
    "        \"\"\"Mark ANY batch as processed (not just 'batch_' files)\"\"\"\n",
    "        self.processed_batches.add(batch_filename)  # Mark ALL files, not just batch_ files\n",
    "        \n",
    "        # Save updated history\n",
    "        batch_history = {\n",
    "            'processed_batches': list(self.processed_batches),\n",
    "            'last_processed': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        batch_history_file = f\"{self.repo_dir}/processed_batches.json\"\n",
    "        with open(batch_history_file, 'w') as f:\n",
    "            json.dump(batch_history, f, indent=2)\n",
    "            \n",
    "        print(f\"‚úÖ Marked batch as processed: {batch_filename}\")\n",
    "    \n",
    "    def save_enhanced_data(self, enhanced_df, original_filename):\n",
    "        \"\"\"Save enhanced data and mark batch as processed\"\"\"\n",
    "        print(\"üíæ Saving enhanced data...\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"enhanced_incremental_{timestamp}.parquet\"\n",
    "        \n",
    "        enhanced_df.to_parquet(output_file)\n",
    "        \n",
    "        # Mark the original batch as processed\n",
    "        self.mark_batch_processed(original_filename)\n",
    "        \n",
    "        print(f\"‚úÖ Enhanced data saved: {output_file}\")\n",
    "        print(f\"üìä Sentiment coverage: {enhanced_df['ml_sentiment_score'].notna().sum()}/{len(enhanced_df)}\")\n",
    "        print(f\"üìä Emotion coverage: {enhanced_df['dominant_emotion'].notna().sum()}/{len(enhanced_df)}\")\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "# =============================================================================\n",
    "# CLASS 2: ML Sentiment Processor \n",
    "# =============================================================================\n",
    "\n",
    "class MLSentimentProcessor:\n",
    "    \"\"\"ML processor with FIXED sentiment label conversion\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.roberta_model = None\n",
    "        self.emotion_model = None\n",
    "        self.setup_models()\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize ML models\"\"\"\n",
    "        print(\"üß† Loading ML models...\")\n",
    "        \n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        print(f\"üéÆ Device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "        \n",
    "        # Load RoBERTa\n",
    "        self.roberta_model = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            device=device,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            batch_size=4,\n",
    "            return_all_scores=True\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ RoBERTa loaded\")\n",
    "        \n",
    "        # Load emotion model\n",
    "        self.emotion_model = pipeline(\n",
    "            \"text-classification\", \n",
    "            model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "            device=device,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            batch_size=4,\n",
    "            top_k=None\n",
    "        )\n",
    "        print(\"‚úÖ Emotion model loaded\")\n",
    "    \n",
    "    def clean_texts(self, texts):\n",
    "        \"\"\"Clean and validate texts\"\"\"\n",
    "        cleaned = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if text and len(str(text).strip()) >= 10:\n",
    "                clean_text = str(text).strip()[:500]  # Truncate long texts\n",
    "                cleaned.append(clean_text)\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        print(f\"‚úÖ Cleaned {len(cleaned)} valid texts from {len(texts)} total\")\n",
    "        return cleaned, valid_indices\n",
    "    \n",
    "    def process_sentiment(self, texts):\n",
    "        \"\"\"Process sentiment in chunks\"\"\"\n",
    "        print(\"ü§ñ Processing sentiment...\")\n",
    "        \n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Process in chunks\n",
    "        chunk_size = 50\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(0, len(texts), chunk_size):\n",
    "            chunk = texts[i:i+chunk_size]\n",
    "            print(f\"   Chunk {i//chunk_size + 1}/{(len(texts)-1)//chunk_size + 1}\")\n",
    "            \n",
    "            try:\n",
    "                chunk_results = self.roberta_model(chunk)\n",
    "                all_results.extend(chunk_results)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Chunk failed: {e}\")\n",
    "                # Add empty results for failed chunk\n",
    "                all_results.extend([[{'label': 'neutral', 'score': 1.0}]] * len(chunk))\n",
    "        \n",
    "        print(f\"‚úÖ Sentiment processing complete: {len(all_results)} results\")\n",
    "        return all_results\n",
    "    \n",
    "    def process_emotions(self, texts):\n",
    "        \"\"\"Process emotions in chunks\"\"\"\n",
    "        print(\"üòä Processing emotions...\")\n",
    "        \n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        chunk_size = 50\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(0, len(texts), chunk_size):\n",
    "            chunk = texts[i:i+chunk_size]\n",
    "            print(f\"   Emotion chunk {i//chunk_size + 1}/{(len(texts)-1)//chunk_size + 1}\")\n",
    "            \n",
    "            try:\n",
    "                chunk_results = self.emotion_model(chunk)\n",
    "                all_results.extend(chunk_results)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Emotion chunk failed: {e}\")\n",
    "                all_results.extend([[]] * len(chunk))\n",
    "        \n",
    "        print(f\"‚úÖ Emotion processing complete: {len(all_results)} results\")\n",
    "        return all_results\n",
    "    \n",
    "    def enhance_dataframe(self, df):\n",
    "        \"\"\"Complete enhancement with FIXED sentiment conversion\"\"\"\n",
    "        print(f\"‚ö° Enhancing {len(df)} posts with ML...\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Prepare texts - handle missing text_cleaned column\n",
    "        if 'text_cleaned' in df.columns:\n",
    "            texts = df['text_cleaned'].fillna('').tolist()\n",
    "        elif 'text' in df.columns:\n",
    "            texts = df['text'].fillna('').tolist()\n",
    "        else:\n",
    "            print(\"‚ùå No text column found\")\n",
    "            return df\n",
    "            \n",
    "        cleaned_texts, valid_indices = self.clean_texts(texts)\n",
    "        \n",
    "        if not cleaned_texts:\n",
    "            print(\"‚ùå No valid texts\")\n",
    "            return df\n",
    "        \n",
    "        # Process with ML models\n",
    "        sentiment_results = self.process_sentiment(cleaned_texts)\n",
    "        emotion_results = self.process_emotions(cleaned_texts)\n",
    "        \n",
    "        # Calculate timing\n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        speed = len(cleaned_texts) / processing_time if processing_time > 0 else 0\n",
    "        \n",
    "        print(f\"‚è±Ô∏è Processing time: {processing_time:.2f}s ({speed:.1f} posts/sec)\")\n",
    "        \n",
    "        # Apply FIXED sentiment conversion\n",
    "        enhanced_df = self.apply_ml_results_FIXED(\n",
    "            df, sentiment_results, emotion_results, valid_indices, speed\n",
    "        )\n",
    "        \n",
    "        return enhanced_df\n",
    "    \n",
    "    def apply_ml_results_FIXED(self, df, sentiment_results, emotion_results, valid_indices, speed):\n",
    "        \"\"\"‚úÖ COMPLETELY FIXED: Handle list of scores correctly + DataFrame reference\"\"\"\n",
    "        print(\"üìä Applying CONTINUOUS sentiment results (FIXED)...\")\n",
    "        \n",
    "        enhanced_df = df.copy()\n",
    "        \n",
    "        # Initialize ML columns\n",
    "        enhanced_df['ml_sentiment_score'] = None\n",
    "        enhanced_df['ml_sentiment_label'] = None\n",
    "        enhanced_df['dominant_emotion'] = None\n",
    "        enhanced_df['emotion_confidence'] = None\n",
    "        \n",
    "        # ‚úÖ FIXED: Process sentiment results (handle list format)\n",
    "        if sentiment_results and len(sentiment_results) == len(valid_indices):\n",
    "            pos_count = neg_count = neu_count = 0\n",
    "            \n",
    "            for i, all_scores in enumerate(sentiment_results):\n",
    "                original_index = valid_indices[i]\n",
    "                \n",
    "                # ‚úÖ FIXED: all_scores is a list like:\n",
    "                # [{'label': 'negative', 'score': 0.1}, \n",
    "                #  {'label': 'neutral', 'score': 0.2}, \n",
    "                #  {'label': 'positive', 'score': 0.7}]\n",
    "                \n",
    "                pos_prob = neg_prob = neu_prob = 0.0\n",
    "                \n",
    "                for score_dict in all_scores:\n",
    "                    label = score_dict['label'].lower()\n",
    "                    prob = score_dict['score']\n",
    "                    \n",
    "                    if label == 'positive':\n",
    "                        pos_prob = prob\n",
    "                    elif label == 'negative':\n",
    "                        neg_prob = prob\n",
    "                    elif label == 'neutral':\n",
    "                        neu_prob = prob\n",
    "                \n",
    "                # ‚úÖ CONTINUOUS: Real sentiment score (-1 to +1)\n",
    "                continuous_score = pos_prob - neg_prob\n",
    "                \n",
    "                # Optional: Reduce if neutral is very confident\n",
    "                if neu_prob > 0.8:  # Very confident neutral\n",
    "                    continuous_score = continuous_score * 0.3\n",
    "                \n",
    "                # ‚úÖ AGGRESSIVE: Apply final labeling\n",
    "                if continuous_score >= 0.01:     \n",
    "                    final_label = 'positive'\n",
    "                    pos_count += 1\n",
    "                elif continuous_score <= -0.01:  \n",
    "                    final_label = 'negative'\n",
    "                    neg_count += 1\n",
    "                else:                            \n",
    "                    final_label = 'neutral'\n",
    "                    neu_count += 1\n",
    "                \n",
    "                enhanced_df.at[original_index, 'ml_sentiment_score'] = continuous_score\n",
    "                enhanced_df.at[original_index, 'ml_sentiment_label'] = final_label\n",
    "            \n",
    "            print(f\"üéØ CONTINUOUS conversion: {pos_count} positive, {neg_count} negative, {neu_count} neutral\")\n",
    "            \n",
    "            # Show the improvement!\n",
    "            scores = enhanced_df['ml_sentiment_score'].dropna()\n",
    "            exact_zeros = (scores == 0.0).sum()\n",
    "            print(f\"üìä Score range: {scores.min():.3f} to {scores.max():.3f}\")\n",
    "            print(f\"üìä Exact zeros: {exact_zeros} (should be much fewer!)\")\n",
    "        \n",
    "        # Process emotions (unchanged)\n",
    "        if emotion_results and len(emotion_results) == len(valid_indices):\n",
    "            for i, emotions in enumerate(emotion_results):\n",
    "                original_index = valid_indices[i]\n",
    "                \n",
    "                if emotions:\n",
    "                    top_emotion = max(emotions, key=lambda x: x['score'])\n",
    "                    enhanced_df.at[original_index, 'dominant_emotion'] = top_emotion['label']\n",
    "                    enhanced_df.at[original_index, 'emotion_confidence'] = top_emotion['score']\n",
    "        \n",
    "        # Add metadata\n",
    "        enhanced_df['ml_processed_at'] = datetime.now()\n",
    "        enhanced_df['processing_speed'] = speed\n",
    "        enhanced_df['processed_by'] = 'continuous_sentiment_fixed'\n",
    "        \n",
    "        # ‚úÖ FIXED: Use enhanced_df, not df\n",
    "        sentiment_count = enhanced_df['ml_sentiment_score'].notna().sum()\n",
    "        emotion_count = enhanced_df['dominant_emotion'].notna().sum()\n",
    "        \n",
    "        print(f\"‚úÖ CONTINUOUS enhancement complete:\")\n",
    "        print(f\"   Sentiment: {sentiment_count}/{len(enhanced_df)} posts\")  # FIXED: enhanced_df\n",
    "        print(f\"   Emotions: {emotion_count}/{len(enhanced_df)} posts\")    # FIXED: enhanced_df\n",
    "        \n",
    "        return enhanced_df\n",
    "\n",
    "# =============================================================================\n",
    "# CLASS 3: GitHub Enhanced Uploader (Fixedreturn types\n",
    "# =============================================================================\n",
    "\n",
    "class SmartGitHubEnhancedUploader:\n",
    "    \"\"\"‚úÖ SMART: Upload to 'new' folder, move to 'processed' after Databricks import\"\"\"\n",
    "    \n",
    "    def __init__(self, username, repo_name):\n",
    "        self.username = username\n",
    "        self.repo_name = repo_name\n",
    "        self.token = token or os.environ.get('GITHUB_TOKEN', 'your_github_token')\n",
    "        self.api_base = f\"https://api.github.com/repos/{username}/{repo_name}\"\n",
    "        \n",
    "        if self.token == 'your_github_token':\n",
    "            print(\"‚ö†Ô∏è WARNING: Using placeholder token - GitHub upload will fail\")\n",
    "        else:\n",
    "            print(\"‚úÖ GitHub token configured\")\n",
    "        \n",
    "    def upload_enhanced_batch(self, enhanced_df, original_batch_filename):\n",
    "        \"\"\"‚úÖ SMART: Upload to 'new' folder for Databricks to discover\"\"\"\n",
    "        \n",
    "        print(f\"üì§ Uploading enhanced data to GitHub NEW folder...\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare enhanced data for upload\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            enhanced_filename = f\"enhanced_{timestamp}.json\"\n",
    "            \n",
    "            total_posts = len(enhanced_df)\n",
    "            \n",
    "            enhanced_data = {\n",
    "                'enhancement_info': {\n",
    "                    'timestamp': timestamp,\n",
    "                    'original_batch': original_batch_filename,\n",
    "                    'enhanced_post_count': total_posts,\n",
    "                    'ml_processing_complete': True,\n",
    "                    'college_cluster_processing': {\n",
    "                        'sentiment_coverage': int(enhanced_df['ml_sentiment_score'].notna().sum()),\n",
    "                        'emotion_coverage': int(enhanced_df['dominant_emotion'].notna().sum()),\n",
    "                        'processing_speed': float(enhanced_df['processing_speed'].iloc[0]) if total_posts > 0 else 0,\n",
    "                        'processed_by': enhanced_df['processed_by'].iloc[0] if total_posts > 0 else 'unknown'\n",
    "                    },\n",
    "                    'databricks_status': 'waiting_for_import'  # Track import status\n",
    "                },\n",
    "                'enhanced_posts': enhanced_df.fillna('').to_dict('records')\n",
    "            }\n",
    "            \n",
    "            # ‚úÖ SMART: Upload to 'new' folder instead of 'enhanced'\n",
    "            success = self.upload_to_github_new_folder(enhanced_data, enhanced_filename)\n",
    "            \n",
    "            if success:\n",
    "                # Update the manifest to track new uploads\n",
    "                self.update_new_upload_manifest(timestamp, enhanced_data['enhancement_info'])\n",
    "                print(f\"‚úÖ Enhanced data uploaded to NEW folder: {enhanced_filename}\")\n",
    "                return enhanced_filename\n",
    "            else:\n",
    "                print(\"‚ùå Upload to GitHub failed\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Enhanced upload error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def upload_to_github_new_folder(self, enhanced_data, filename):\n",
    "        \"\"\"Upload enhanced data to GitHub 'new' folder for Databricks discovery\"\"\"\n",
    "        try:\n",
    "            # Convert to JSON and encode\n",
    "            json_content = json.dumps(enhanced_data, indent=2, default=str)\n",
    "            encoded_content = base64.b64encode(json_content.encode()).decode()\n",
    "            \n",
    "            # ‚úÖ SMART: Upload to 'new' folder instead of 'enhanced'\n",
    "            filepath = f\"data/enhanced/new/{filename}\"  # NEW path structure\n",
    "            \n",
    "            headers = {\n",
    "                \"Authorization\": f\"token {self.token}\",\n",
    "                \"Accept\": \"application/vnd.github.v3+json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"message\": f\"NEW enhanced data: {enhanced_data['enhancement_info']['enhanced_post_count']} posts\",\n",
    "                \"content\": encoded_content,\n",
    "                \"branch\": \"main\"\n",
    "            }\n",
    "            \n",
    "            upload_url = f\"{self.api_base}/contents/{filepath}\"\n",
    "            response = requests.put(upload_url, headers=headers, json=payload)\n",
    "            \n",
    "            if response.status_code in [200, 201]:\n",
    "                print(f\"‚úÖ Uploaded to GitHub NEW folder: {filepath}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå GitHub upload failed: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå GitHub upload error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def update_new_upload_manifest(self, timestamp, enhancement_info):\n",
    "        \"\"\"Update manifest to track files waiting for Databricks import\"\"\"\n",
    "        \n",
    "        new_upload_manifest = {\n",
    "            'pending_imports': [\n",
    "                {\n",
    "                    'timestamp': timestamp,\n",
    "                    'filename': f\"enhanced_{timestamp}.json\",\n",
    "                    'post_count': enhancement_info['enhanced_post_count'],\n",
    "                    'original_batch': enhancement_info['original_batch'],\n",
    "                    'status': 'waiting_for_databricks_import',\n",
    "                    'uploaded_at': datetime.now().isoformat()\n",
    "                }\n",
    "            ],\n",
    "            'databricks_import_instructions': {\n",
    "                'new_files_location': 'data/enhanced/new/',\n",
    "                'processed_files_location': 'data/enhanced/processed/',\n",
    "                'table_target': 'social_media.bluesky_enhanced_clean',\n",
    "                'import_type': 'incremental_append'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Get existing manifest or create new\n",
    "            manifest_url = f\"{self.api_base}/contents/data/enhanced/new_uploads_manifest.json\"\n",
    "            headers = {\"Authorization\": f\"token {self.token}\"}\n",
    "            \n",
    "            # Check if manifest exists\n",
    "            check_response = requests.get(manifest_url, headers=headers)\n",
    "            \n",
    "            if check_response.status_code == 200:\n",
    "                # Merge with existing manifest\n",
    "                existing_content = check_response.json()['content']\n",
    "                existing_manifest = json.loads(base64.b64decode(existing_content).decode('utf-8'))\n",
    "                \n",
    "                # Add to existing pending imports\n",
    "                existing_manifest['pending_imports'].extend(new_upload_manifest['pending_imports'])\n",
    "                \n",
    "                # Keep only last 50 imports to avoid huge manifest\n",
    "                existing_manifest['pending_imports'] = existing_manifest['pending_imports'][-50:]\n",
    "                \n",
    "                new_upload_manifest = existing_manifest\n",
    "                sha = check_response.json()[\"sha\"]\n",
    "            else:\n",
    "                sha = None\n",
    "            \n",
    "            # Upload updated manifest\n",
    "            manifest_content = json.dumps(new_upload_manifest, indent=2, default=str)\n",
    "            encoded_manifest = base64.b64encode(manifest_content.encode()).decode()\n",
    "            \n",
    "            payload = {\n",
    "                \"message\": f\"Track new upload: enhanced_{timestamp}.json\",\n",
    "                \"content\": encoded_manifest,\n",
    "                \"branch\": \"main\"\n",
    "            }\n",
    "            \n",
    "            if sha:\n",
    "                payload[\"sha\"] = sha\n",
    "            \n",
    "            response = requests.put(manifest_url, headers=headers, json=payload)\n",
    "            \n",
    "            if response.status_code in [200, 201]:\n",
    "                print(\"‚úÖ New upload manifest updated\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Manifest update failed: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Manifest error: {e}\")\n",
    "    \n",
    "    def move_file_to_processed(self, filename):\n",
    "        \"\"\"Move file from 'new' to 'processed' after Databricks import\"\"\"\n",
    "        try:\n",
    "            headers = {\"Authorization\": f\"token {self.token}\"}\n",
    "            \n",
    "            # 1. Get file from 'new' folder\n",
    "            new_file_url = f\"{self.api_base}/contents/data/enhanced/new/{filename}\"\n",
    "            response = requests.get(new_file_url, headers=headers)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Could not find file in NEW folder: {filename}\")\n",
    "                return False\n",
    "            \n",
    "            file_data = response.json()\n",
    "            file_content = file_data['content']\n",
    "            file_sha = file_data['sha']\n",
    "            \n",
    "            # 2. Create file in 'processed' folder\n",
    "            processed_file_url = f\"{self.api_base}/contents/data/enhanced/processed/{filename}\"\n",
    "            \n",
    "            upload_payload = {\n",
    "                \"message\": f\"Move to processed: {filename}\",\n",
    "                \"content\": file_content,\n",
    "                \"branch\": \"main\"\n",
    "            }\n",
    "            \n",
    "            upload_response = requests.put(processed_file_url, headers=headers, json=upload_payload)\n",
    "            \n",
    "            if upload_response.status_code not in [200, 201]:\n",
    "                print(f\"‚ùå Could not create file in PROCESSED folder\")\n",
    "                return False\n",
    "            \n",
    "            # 3. Delete file from 'new' folder\n",
    "            delete_payload = {\n",
    "                \"message\": f\"Remove from new folder: {filename}\",\n",
    "                \"sha\": file_sha,\n",
    "                \"branch\": \"main\"\n",
    "            }\n",
    "            \n",
    "            delete_response = requests.delete(new_file_url, headers=headers, json=delete_payload)\n",
    "            \n",
    "            if delete_response.status_code == 200:\n",
    "                print(f\"‚úÖ Moved {filename}: new ‚Üí processed\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è File copied but not deleted from NEW folder\")\n",
    "                return True  # File is still accessible in processed\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Move file error: {e}\")\n",
    "            return False\n",
    "\n",
    "# =============================================================================\n",
    "# CLASS 4: Complete Pipeline Orchestrator (Fixed variable usage)\n",
    "# =============================================================================\n",
    "\n",
    "class CompletePipelineOrchestratorWithUpload:\n",
    "    \"\"\"‚úÖ UPDATED: Complete pipeline that processes ALL batches automatically\"\"\"\n",
    "    \n",
    "    def __init__(self, github_username, repo_name):\n",
    "        print(\"üèóÔ∏è INITIALIZING COMPLETE PIPELINE WITH GITHUB UPLOAD\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.github_manager = UpdatedGitHubDataManager(github_username, repo_name)\n",
    "        self.ml_processor = MLSentimentProcessor()\n",
    "        self.github_uploader = SmartGitHubEnhancedUploader(github_username, repo_name)\n",
    "        \n",
    "        print(\"‚úÖ Pipeline + GitHub uploader initialized!\")\n",
    "    \n",
    "    def run_complete_cycle_with_upload(self):\n",
    "        \"\"\"‚úÖ UPDATED: Process ALL available batches in one run\"\"\"\n",
    "        \n",
    "        print(\"\\nüöÄ COMPLETE CYCLE: PROCESS ALL BATCHES + UPLOAD TO GITHUB\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Step 1: Get ALL unprocessed data\n",
    "        batch_data_list = self.github_manager.get_all_unprocessed_data()\n",
    "        \n",
    "        if not batch_data_list:\n",
    "            print(\"üìã No new data to process - all caught up!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nüéØ PROCESSING {len(batch_data_list)} BATCHES...\")\n",
    "        \n",
    "        # Step 2: Process each batch\n",
    "        all_enhanced_data = []\n",
    "        successful_batches = 0\n",
    "        failed_batches = 0\n",
    "        \n",
    "        for i, (df, filename) in enumerate(batch_data_list, 1):\n",
    "            print(f\"\\nüì¶ BATCH {i}/{len(batch_data_list)}: {filename}\")\n",
    "            print(f\"   Posts in batch: {len(df)}\")\n",
    "            \n",
    "            try:\n",
    "                # ML enhancement\n",
    "                enhanced_df = self.ml_processor.enhance_dataframe(df)\n",
    "                \n",
    "                # Check if processing was successful\n",
    "                sentiment_count = enhanced_df['ml_sentiment_score'].notna().sum()\n",
    "                if sentiment_count == 0:\n",
    "                    print(f\"   ‚ùå No sentiment analysis results\")\n",
    "                    failed_batches += 1\n",
    "                    continue\n",
    "                \n",
    "                # Save locally and mark as processed\n",
    "                local_file = self.github_manager.save_enhanced_data(enhanced_df, filename)\n",
    "                \n",
    "                # Upload to GitHub\n",
    "                github_filename = self.github_uploader.upload_enhanced_batch(enhanced_df, filename)\n",
    "                \n",
    "                if github_filename:\n",
    "                    print(f\"   ‚úÖ Successfully processed and uploaded: {github_filename}\")\n",
    "                    successful_batches += 1\n",
    "                    all_enhanced_data.append(enhanced_df)\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Local processing succeeded, GitHub upload failed\")\n",
    "                    successful_batches += 1  # Still count as success\n",
    "                    all_enhanced_data.append(enhanced_df)\n",
    "                \n",
    "                # Small delay to avoid overwhelming APIs\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Batch processing failed: {e}\")\n",
    "                failed_batches += 1\n",
    "                continue\n",
    "        \n",
    "        # Step 3: Summary\n",
    "        print(f\"\\nüìä BATCH PROCESSING SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"   ‚úÖ Successful batches: {successful_batches}\")\n",
    "        print(f\"   ‚ùå Failed batches: {failed_batches}\")\n",
    "        print(f\"   üìà Total processed: {successful_batches + failed_batches}\")\n",
    "        \n",
    "        if successful_batches > 0:\n",
    "            # Combine all enhanced data for analysis\n",
    "            combined_enhanced_df = pd.concat(all_enhanced_data, ignore_index=True)\n",
    "            \n",
    "            total_posts = len(combined_enhanced_df)\n",
    "            sentiment_posts = combined_enhanced_df['ml_sentiment_score'].notna().sum()\n",
    "            \n",
    "            print(f\"\\nüéâ COMPLETE SUCCESS!\")\n",
    "            print(f\"‚úÖ Processed {successful_batches} batches\")\n",
    "            print(f\"‚úÖ Enhanced {total_posts} total posts\")\n",
    "            print(f\"‚úÖ Sentiment analysis: {sentiment_posts}/{total_posts} posts\")\n",
    "            print(f\"‚úÖ All enhanced data uploaded to GitHub\")\n",
    "            print(f\"‚úÖ Ready for Databricks import\")\n",
    "            \n",
    "            return combined_enhanced_df\n",
    "        else:\n",
    "            print(f\"\\n‚ùå No batches were successfully processed\")\n",
    "            return None\n",
    "    \n",
    "    def run_single_batch_cycle(self):\n",
    "        \"\"\"LEGACY: Keep old single-batch method for compatibility\"\"\"\n",
    "        \n",
    "        print(\"\\nüöÄ SINGLE BATCH CYCLE: PROCESS + UPLOAD TO GITHUB\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Get latest data\n",
    "        df, filename = self.github_manager.get_latest_data()\n",
    "        \n",
    "        if df is None:\n",
    "            print(\"üìã No new data to process\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: ML enhancement\n",
    "        print(f\"üéØ Processing {len(df)} posts with ML...\")\n",
    "        enhanced_df = self.ml_processor.enhance_dataframe(df)\n",
    "        \n",
    "        # Step 3: Save locally\n",
    "        local_file = self.github_manager.save_enhanced_data(enhanced_df, filename)\n",
    "        \n",
    "        # Step 4: Upload to GitHub for Databricks\n",
    "        print(\"\\nüì§ Uploading enhanced data to GitHub for Databricks...\")\n",
    "        github_filename = self.github_uploader.upload_enhanced_batch(enhanced_df, filename)\n",
    "        \n",
    "        if github_filename:\n",
    "            print(\"üéâ SINGLE BATCH SUCCESS!\")\n",
    "            print(\"‚úÖ Data processed with ML\")\n",
    "            print(\"‚úÖ Enhanced data uploaded to GitHub\")\n",
    "            print(\"‚úÖ Ready for Databricks import\")\n",
    "            \n",
    "            print(f\"\\nüîç ENHANCED DATA LOCATIONS:\")\n",
    "            print(f\"   Local file: {local_file}\")\n",
    "            print(f\"   GitHub file: data/enhanced/{github_filename}\")\n",
    "            print(f\"   Ready for: Databricks import\")\n",
    "            \n",
    "            return enhanced_df\n",
    "        else:\n",
    "            print(\"‚ùå GitHub upload failed - but local processing succeeded\")\n",
    "            return enhanced_df\n",
    "    \n",
    "    def show_detailed_results(self, enhanced_df):\n",
    "        \"\"\"Show detailed results with sentiment analysis\"\"\"\n",
    "        print(\"\\nüìä DETAILED RESULTS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        total_posts = len(enhanced_df)\n",
    "        sentiment_posts = enhanced_df['ml_sentiment_score'].notna().sum()\n",
    "        emotion_posts = enhanced_df['dominant_emotion'].notna().sum()\n",
    "        \n",
    "        print(f\"üìà Processing Summary:\")\n",
    "        print(f\"   Total posts: {total_posts}\")\n",
    "        print(f\"   Sentiment analysis: {sentiment_posts} posts ({sentiment_posts/total_posts*100:.1f}%)\")\n",
    "        print(f\"   Emotion analysis: {emotion_posts} posts ({emotion_posts/total_posts*100:.1f}%)\")\n",
    "        \n",
    "        # Sentiment distribution\n",
    "        if 'ml_sentiment_label' in enhanced_df.columns:\n",
    "            label_dist = enhanced_df['ml_sentiment_label'].value_counts()\n",
    "            print(f\"\\nüè∑Ô∏è Sentiment Distribution:\")\n",
    "            for label, count in label_dist.items():\n",
    "                percentage = (count / total_posts) * 100\n",
    "                print(f\"   {label}: {count} posts ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Score statistics\n",
    "        scores = enhanced_df['ml_sentiment_score'].dropna()\n",
    "        if len(scores) > 0:\n",
    "            print(f\"\\nüìä Sentiment Score Statistics:\")\n",
    "            print(f\"   Range: {scores.min():.3f} to {scores.max():.3f}\")\n",
    "            print(f\"   Mean: {scores.mean():.3f}\")\n",
    "            print(f\"   Std: {scores.std():.3f}\")\n",
    "            \n",
    "            # Check improvement\n",
    "            exact_zeros = (scores == 0.0).sum()\n",
    "            near_zeros = ((scores >= -0.01) & (scores <= 0.01)).sum()\n",
    "            print(f\"   Exact zeros: {exact_zeros}\")\n",
    "            print(f\"   Near-zero (-0.01 to 0.01): {near_zeros}\")\n",
    "        \n",
    "        # Emotion distribution\n",
    "        if 'dominant_emotion' in enhanced_df.columns:\n",
    "            emotion_dist = enhanced_df['dominant_emotion'].value_counts()\n",
    "            print(f\"\\nüòä Emotion Distribution:\")\n",
    "            for emotion, count in emotion_dist.head().items():\n",
    "                percentage = (count / emotion_posts) * 100 if emotion_posts > 0 else 0\n",
    "                print(f\"   {emotion}: {count} posts ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Sample results\n",
    "        print(f\"\\nüîç Sample Enhanced Posts:\")\n",
    "        sample_cols = ['text', 'ml_sentiment_score', 'ml_sentiment_label', 'dominant_emotion']\n",
    "        available_cols = [col for col in sample_cols if col in enhanced_df.columns]\n",
    "        \n",
    "        if 'text' not in enhanced_df.columns and 'text_cleaned' in enhanced_df.columns:\n",
    "            available_cols = ['text_cleaned'] + [col for col in sample_cols[1:] if col in enhanced_df.columns]\n",
    "        \n",
    "        sample_df = enhanced_df[available_cols].dropna(subset=['ml_sentiment_score']).head(3)\n",
    "        \n",
    "        for idx, row in sample_df.iterrows():\n",
    "            text_col = 'text' if 'text' in row else 'text_cleaned'\n",
    "            if text_col in row:\n",
    "                print(f\"\\nPost {idx+1}:\")\n",
    "                print(f\"   Text: {str(row[text_col])[:100]}...\")\n",
    "                print(f\"   Sentiment: {row['ml_sentiment_score']:.3f} ({row['ml_sentiment_label']})\")\n",
    "                if 'dominant_emotion' in row and pd.notna(row['dominant_emotion']):\n",
    "                    print(f\"   Emotion: {row['dominant_emotion']}\")\n",
    "\n",
    "\n",
    "def run_fixed_pipeline():\n",
    "    \"\"\"‚úÖ UPDATED: Run the complete fixed pipeline - processes ALL batches\"\"\"\n",
    "    \n",
    "    print(\"üöÄ STARTING COMPLETE FIXED COLLEGE CLUSTER PIPELINE\")\n",
    "    print(\"üîÑ NOW PROCESSES ALL AVAILABLE BATCHES AUTOMATICALLY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Configuration\n",
    "    GITHUB_USERNAME = \"AlexanderHuynhKoehler\"\n",
    "    REPO_NAME = \"bluesky-data-pipeline\"\n",
    "    \n",
    "    try:\n",
    "        complete_pipeline = CompletePipelineOrchestratorWithUpload(GITHUB_USERNAME, REPO_NAME)\n",
    "        \n",
    "        # ‚úÖ This will now process ALL available batches\n",
    "        enhanced_results = complete_pipeline.run_complete_cycle_with_upload()\n",
    "\n",
    "        if enhanced_results is not None:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"üéâ COMPLETE PIPELINE SUCCESS!\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            # Show detailed results\n",
    "            complete_pipeline.show_detailed_results(enhanced_results)\n",
    "            \n",
    "            globals()['enhanced_data'] = enhanced_results\n",
    "            print(\"\\nüí° Enhanced data available as 'enhanced_data' variable\")\n",
    "            print(\"   Use: enhanced_data.head() to explore\")\n",
    "            print(\"   Use: enhanced_data['ml_sentiment_label'].value_counts() to see distribution\")\n",
    "            \n",
    "            return enhanced_results\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n‚ùå No data processed - check if new batches are available\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "enhanced_results = run_fixed_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510be21f-ea13-42e7-9403-40913e252817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bluesky Pipeline",
   "language": "python",
   "name": "bluesky-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
